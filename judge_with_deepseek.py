"""
LLM Judge for Base vs SFT Model Comparison using DeepSeek-R1.

This script:
1. Loads comparison results from JSON file (generated by memory_efficient_comparison.py)
2. Loads ground truth solutions from exercises.jsonl
3. Uses DeepSeek-R1 model via Unsloth to judge which model performed better
4. Evaluates based on: correctness, brevity, clear explanation
5. Generates a comprehensive report with statistics

Usage:
    python judge_with_deepseek.py comparison_results_TIMESTAMP.json

Requirements:
    - unsloth installed
    - Sufficient GPU memory for DeepSeek-R1 model
"""

import json
import sys
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import pandas as pd
import torch
from unsloth import FastLanguageModel


class LLMJudge:
    """DeepSeek-R1 judge for model comparison."""

    def __init__(
        self,
        model_name: str = "unsloth/DeepSeek-R1-Distill-Qwen-7B",
        max_seq_length: int = 4096,
        load_in_4bit: bool = True,
    ):
        """
        Initialize LLM judge.

        Args:
            model_name: HuggingFace model ID for judge
            max_seq_length: Maximum sequence length
            load_in_4bit: Whether to load model in 4-bit quantization
        """
        self.model_name = model_name
        self.max_seq_length = max_seq_length

        print(f"ü§ñ Loading judge model: {model_name}")
        print(f"   Max sequence length: {max_seq_length}")
        print(f"   4-bit quantization: {load_in_4bit}")

        # Load model
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=max_seq_length,
            load_in_4bit=load_in_4bit,
            fast_inference=True,
        )

        # Enable inference mode
        FastLanguageModel.for_inference(self.model)

        print(f"‚úÖ Judge model loaded successfully\n")

    def judge_responses(
        self,
        problem: str,
        ground_truth: str,
        base_response: str,
        sft_response: str
    ) -> Dict:
        """
        Judge which model response is better.

        Args:
            problem: The optimization problem
            ground_truth: The correct solution
            base_response: Base model's response
            sft_response: Fine-tuned model's response

        Returns:
            Dictionary with judgment results including scores and feedback
        """
        judge_prompt = f"""You are an expert judge evaluating two AI models' solutions to a convex optimization problem.

**Problem:**
{problem}

**Ground Truth Solution:**
{ground_truth}

**Model A Response (Base Model):**
{base_response}

**Model B Response (Fine-tuned Model):**
{sft_response}

**Evaluation Criteria:**

1. **Correctness** (50%): Is the solution mathematically correct compared to ground truth?
   - Check if the answer matches the ground truth
   - Verify the reasoning steps are valid
   - Assess if the approach is sound

2. **Brevity** (25%): Is the solution appropriately concise?
   - Not overly verbose
   - No unnecessary repetition
   - Gets to the point efficiently

3. **Clear Explanation** (25%): Is the reasoning clear and easy to follow?
   - Logical flow of steps
   - Well-explained reasoning
   - Easy to understand

**Your Task:**
Carefully evaluate both responses according to the criteria above. For each model:
1. Provide a score (0-10) for each criterion
2. Calculate weighted total score
3. Give brief feedback on strengths and weaknesses
4. Declare a winner

**Output Format (JSON only, no other text):**
{{
  "model_a_scores": {{
    "correctness": <score 0-10>,
    "brevity": <score 0-10>,
    "clear_explanation": <score 0-10>,
    "total": <weighted total 0-10>
  }},
  "model_b_scores": {{
    "correctness": <score 0-10>,
    "brevity": <score 0-10>,
    "clear_explanation": <score 0-10>,
    "total": <weighted total 0-10>
  }},
  "model_a_feedback": "<brief feedback on strengths and weaknesses>",
  "model_b_feedback": "<brief feedback on strengths and weaknesses>",
  "winner": "<Model A|Model B|Tie>",
  "reason": "<1-2 sentence explanation of decision>"
}}

Think carefully and respond ONLY with the JSON output."""

        # Generate judgment
        try:
            # Format as chat (DeepSeek-R1 expects specific format)
            messages = [
                {"role": "user", "content": judge_prompt}
            ]

            # Apply chat template if available
            if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                )
            else:
                # Fallback: simple format
                text = judge_prompt

            # Tokenize
            inputs = self.tokenizer(text, return_tensors="pt").to("cuda")

            # Generate with appropriate settings for DeepSeek-R1
            # DeepSeek-R1 works best with temperature 0.5-0.7
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=2048,
                temperature=0.6,
                top_p=0.95,
                do_sample=True,
            )

            # Decode response
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            # Parse JSON from response
            judgment_text = response.strip()

            # Handle case where model might wrap JSON in markdown code blocks or <think> tags
            if "```json" in judgment_text:
                judgment_text = judgment_text.split("```json")[1].split("```")[0]
            elif "```" in judgment_text:
                judgment_text = judgment_text.split("```")[1].split("```")[0]

            # Remove <think> tags if present (DeepSeek-R1 thinking process)
            if "<think>" in judgment_text and "</think>" in judgment_text:
                # Extract content after thinking
                judgment_text = judgment_text.split("</think>")[-1].strip()

            # Try to find JSON object
            start_idx = judgment_text.find('{')
            end_idx = judgment_text.rfind('}')
            if start_idx != -1 and end_idx != -1:
                judgment_text = judgment_text[start_idx:end_idx+1]

            judgment = json.loads(judgment_text)
            return judgment

        except Exception as e:
            print(f"‚ö†Ô∏è Error during judgment: {e}")
            print(f"Raw response: {response[:500] if 'response' in locals() else 'N/A'}")

            # Return a default judgment in case of error
            return {
                "model_a_scores": {"total": 0},
                "model_b_scores": {"total": 0},
                "winner": "Error",
                "reason": f"Error during judgment: {str(e)}"
            }


def load_ground_truth(exercises_file: str = "exercises.jsonl") -> Dict[str, str]:
    """
    Load ground truth solutions from exercises.jsonl.

    Args:
        exercises_file: Path to exercises.jsonl

    Returns:
        Dictionary mapping exercise_text to solution_text
    """
    ground_truth = {}
    with open(exercises_file, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            ground_truth[data['exercise_text']] = data['solution_text']
    return ground_truth


def save_dataframe(
    results: List[Dict],
    output_file: str
):
    """
    Save judgment results to dataframe (CSV and pickle).

    Args:
        results: List of judgment results
        output_file: Output file path (without extension)
    """
    # Build dataframe
    df_data = []
    for i, result in enumerate(results, 1):
        judgment = result['judgment']

        row = {
            'problem_id': i,
            'question': result['problem'],
            'ground_truth_answer': result.get('ground_truth', 'N/A'),
            'base_model_answer': result['base_response'],
            'sft_model_answer': result['sft_response'],
            'judge_response_base': judgment.get('model_a_feedback', 'N/A'),
            'judge_response_sft': judgment.get('model_b_feedback', 'N/A'),
            'base_correctness': judgment['model_a_scores'].get('correctness', 0),
            'base_brevity': judgment['model_a_scores'].get('brevity', 0),
            'base_clear_explanation': judgment['model_a_scores'].get('clear_explanation', 0),
            'base_total_score': judgment['model_a_scores'].get('total', 0),
            'sft_correctness': judgment['model_b_scores'].get('correctness', 0),
            'sft_brevity': judgment['model_b_scores'].get('brevity', 0),
            'sft_clear_explanation': judgment['model_b_scores'].get('clear_explanation', 0),
            'sft_total_score': judgment['model_b_scores'].get('total', 0),
            'winner': judgment.get('winner', 'N/A'),
            'reason': judgment.get('reason', 'N/A'),
        }
        df_data.append(row)

    df = pd.DataFrame(df_data)

    # Save to CSV
    csv_file = f"{output_file}.csv"
    df.to_csv(csv_file, index=False)
    print(f"üìä Saved dataframe to: {csv_file}")

    # Save to pickle for easy loading
    pickle_file = f"{output_file}.pkl"
    df.to_pickle(pickle_file)
    print(f"üì¶ Saved dataframe to: {pickle_file}")


def generate_report(
    results: List[Dict],
    base_model: str,
    sft_model: str,
    judge_model: str,
    output_file: str
):
    """
    Generate markdown report of judgment results.

    Args:
        results: List of judgment results
        base_model: Name of base model
        sft_model: Name of fine-tuned model
        judge_model: Name of judge model
        output_file: Output file path
    """
    # Calculate statistics
    base_wins = sum(1 for r in results if r['judgment']['winner'] == 'Model A')
    sft_wins = sum(1 for r in results if r['judgment']['winner'] == 'Model B')
    ties = sum(1 for r in results if r['judgment']['winner'] == 'Tie')
    errors = sum(1 for r in results if r['judgment']['winner'] == 'Error')

    total_problems = len(results)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# LLM Judge Evaluation Report: Base vs Fine-tuned Model\n\n")
        f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # Model information
        f.write("## Models Evaluated\n\n")
        f.write(f"- **Base Model:** {base_model}\n")
        f.write(f"- **Fine-tuned Model:** {sft_model}\n")
        f.write(f"- **Judge Model:** {judge_model}\n\n")

        # Summary statistics
        f.write("## Summary Statistics\n\n")
        f.write(f"| Outcome | Count | Percentage |\n")
        f.write(f"|---------|-------|------------|\n")
        f.write(f"| **SFT Model Wins** | {sft_wins} | {sft_wins/total_problems*100:.1f}% |\n")
        f.write(f"| Base Model Wins | {base_wins} | {base_wins/total_problems*100:.1f}% |\n")
        f.write(f"| Ties | {ties} | {ties/total_problems*100:.1f}% |\n")
        if errors > 0:
            f.write(f"| Errors | {errors} | {errors/total_problems*100:.1f}% |\n")
        f.write(f"| **Total Problems** | {total_problems} | 100% |\n\n")

        # Key findings
        f.write("## Key Findings\n\n")
        if sft_wins > base_wins:
            margin = sft_wins - base_wins
            f.write(f"üéâ **Fine-tuned model outperformed base model** by {margin} problems ")
            f.write(f"({margin/total_problems*100:.1f}% margin).\n\n")
        elif base_wins > sft_wins:
            margin = base_wins - sft_wins
            f.write(f"‚ö†Ô∏è **Base model outperformed fine-tuned model** by {margin} problems ")
            f.write(f"({margin/total_problems*100:.1f}% margin). This is unexpected.\n\n")
        else:
            f.write(f"‚ûñ **Models performed equally** with {sft_wins} wins each.\n\n")

        # Average scores
        avg_base_score = sum(r['judgment']['model_a_scores']['total'] for r in results) / total_problems
        avg_sft_score = sum(r['judgment']['model_b_scores']['total'] for r in results) / total_problems

        f.write("## Average Scores (0-10 scale)\n\n")
        f.write(f"- **Base Model:** {avg_base_score:.2f}\n")
        f.write(f"- **Fine-tuned Model:** {avg_sft_score:.2f}\n")
        f.write(f"- **Difference:** {avg_sft_score - avg_base_score:+.2f}\n\n")

        # Detailed results
        f.write("---\n\n")
        f.write("## Detailed Problem-by-Problem Results\n\n")

        for i, result in enumerate(results, 1):
            judgment = result['judgment']
            winner = judgment['winner']

            f.write(f"### Problem {i}\n\n")

            # Problem statement
            f.write(f"**Problem:**\n")
            f.write(f"```\n{result['problem'][:200]}...\n```\n\n")

            # Scores
            f.write(f"**Scores:**\n\n")
            f.write(f"| Model | Correctness (50%) | Brevity (25%) | Clarity (25%) | **Total** |\n")
            f.write(f"|-------|-------------------|---------------|---------------|--------|\n")

            model_a = judgment['model_a_scores']
            model_b = judgment['model_b_scores']

            f.write(f"| Base | {model_a.get('correctness', 'N/A')} | ")
            f.write(f"{model_a.get('brevity', 'N/A')} | ")
            f.write(f"{model_a.get('clear_explanation', 'N/A')} | ")
            f.write(f"**{model_a['total']:.1f}** |\n")

            f.write(f"| SFT | {model_b.get('correctness', 'N/A')} | ")
            f.write(f"{model_b.get('brevity', 'N/A')} | ")
            f.write(f"{model_b.get('clear_explanation', 'N/A')} | ")
            f.write(f"**{model_b['total']:.1f}** |\n\n")

            # Winner
            winner_emoji = "üü¢" if winner == "Model B" else ("üîµ" if winner == "Model A" else "‚ö™")
            f.write(f"**Winner:** {winner_emoji} {winner}\n\n")

            # Feedback
            f.write(f"**Base Model Feedback:** {judgment.get('model_a_feedback', 'N/A')}\n\n")
            f.write(f"**SFT Model Feedback:** {judgment.get('model_b_feedback', 'N/A')}\n\n")
            f.write(f"**Reason:** {judgment.get('reason', 'N/A')}\n\n")

            f.write("---\n\n")

        # Final summary
        f.write("## Conclusion\n\n")
        if sft_wins > base_wins:
            f.write(f"‚úÖ The fine-tuned model demonstrates clear improvement over the base model, ")
            f.write(f"winning {sft_wins}/{total_problems} comparisons ({sft_wins/total_problems*100:.1f}%).\n")
        elif base_wins > sft_wins:
            f.write(f"‚ö†Ô∏è The fine-tuned model performed worse than the base model, ")
            f.write(f"winning only {sft_wins}/{total_problems} comparisons ({sft_wins/total_problems*100:.1f}%). ")
            f.write(f"This suggests the fine-tuning may need adjustment.\n")
        else:
            f.write(f"The models performed comparably, each winning {sft_wins}/{total_problems} comparisons.\n")


def main():
    parser = argparse.ArgumentParser(
        description="Judge base vs SFT model comparison using DeepSeek-R1"
    )
    parser.add_argument(
        "comparison_file",
        help="Path to comparison results JSON file"
    )
    parser.add_argument(
        "--exercises",
        default="exercises.jsonl",
        help="Path to exercises.jsonl (default: exercises.jsonl)"
    )
    parser.add_argument(
        "--judge-model",
        default="unsloth/DeepSeek-R1-Distill-Qwen-7B",
        help="Judge model ID (default: unsloth/DeepSeek-R1-Distill-Qwen-7B)"
    )
    parser.add_argument(
        "--max-seq-length",
        type=int,
        default=4096,
        help="Maximum sequence length (default: 4096)"
    )
    parser.add_argument(
        "--load-in-4bit",
        action="store_true",
        default=True,
        help="Load model in 4-bit quantization (default: True)"
    )

    args = parser.parse_args()

    # Check if comparison file exists
    if not Path(args.comparison_file).exists():
        print(f"‚ùå Error: Comparison file not found: {args.comparison_file}")
        sys.exit(1)

    print("\n" + "="*80)
    print("ü§ñ LLM JUDGE: Base vs SFT Model Evaluation")
    print("="*80)

    # Load comparison results
    print(f"\nüìÇ Loading comparison results from: {args.comparison_file}")
    with open(args.comparison_file, 'r', encoding='utf-8') as f:
        comparison_data = json.load(f)

    # Load ground truth
    print(f"üìö Loading ground truth from: {args.exercises}")
    ground_truth = load_ground_truth(args.exercises)

    # Initialize judge
    print(f"\n‚öñÔ∏è Initializing LLM judge...\n")
    judge = LLMJudge(
        model_name=args.judge_model,
        max_seq_length=args.max_seq_length,
        load_in_4bit=args.load_in_4bit
    )

    # Get model names from metadata
    base_model = comparison_data['metadata']['base_model']
    sft_model = comparison_data['metadata']['sft_model']

    print(f"üìä Evaluating {len(comparison_data['results'])} problems...")
    print(f"   Base Model: {base_model}")
    print(f"   SFT Model: {sft_model}\n")

    # Judge each problem
    judged_results = []
    for i, result in enumerate(comparison_data['results'], 1):
        problem = result['problem']
        base_response = result['base_model']['response']
        sft_response = result['sft_model']['response']

        # Get ground truth for this problem
        truth = ground_truth.get(problem, "Ground truth not found")

        print(f"   Problem {i}/{len(comparison_data['results'])}...", end=' ', flush=True)

        # Judge
        judgment = judge.judge_responses(
            problem=problem,
            ground_truth=truth,
            base_response=base_response,
            sft_response=sft_response
        )

        winner_symbol = "üü¢" if judgment['winner'] == "Model B" else ("üîµ" if judgment['winner'] == "Model A" else "‚ö™")
        print(f"{winner_symbol} {judgment['winner']}")

        judged_results.append({
            'problem': problem,
            'ground_truth': truth,
            'base_response': base_response,
            'sft_response': sft_response,
            'judgment': judgment
        })

    # Generate report
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"deepseek_judge_report_{timestamp}.md"

    print(f"\nüìù Generating report: {report_file}")
    generate_report(
        results=judged_results,
        base_model=base_model,
        sft_model=sft_model,
        judge_model=args.judge_model,
        output_file=report_file
    )

    # Save dataframe
    df_file = f"deepseek_judge_dataframe_{timestamp}"
    print(f"üíæ Saving dataframe...")
    save_dataframe(
        results=judged_results,
        output_file=df_file
    )

    # Print summary to console
    base_wins = sum(1 for r in judged_results if r['judgment']['winner'] == 'Model A')
    sft_wins = sum(1 for r in judged_results if r['judgment']['winner'] == 'Model B')
    ties = sum(1 for r in judged_results if r['judgment']['winner'] == 'Tie')

    print("\n" + "="*80)
    print("üìä FINAL RESULTS")
    print("="*80)
    print(f"\nüü¢ SFT Model Wins:  {sft_wins}/{len(judged_results)} ({sft_wins/len(judged_results)*100:.1f}%)")
    print(f"üîµ Base Model Wins: {base_wins}/{len(judged_results)} ({base_wins/len(judged_results)*100:.1f}%)")
    print(f"‚ö™ Ties:            {ties}/{len(judged_results)} ({ties/len(judged_results)*100:.1f}%)")

    if sft_wins > base_wins:
        print(f"\nüéâ Fine-tuned model is BETTER (+{sft_wins - base_wins} wins)")
    elif base_wins > sft_wins:
        print(f"\n‚ö†Ô∏è Base model is BETTER (+{base_wins - sft_wins} wins)")
    else:
        print(f"\n‚ûñ Models performed EQUALLY")

    print("\n" + "="*80)
    print("‚úÖ COMPLETE!")
    print("="*80)
    print(f"\nüìÑ View detailed report: {report_file}")
    print(f"üìä View dataframe: {df_file}.csv")
    print(f"üì¶ Load dataframe: pd.read_pickle('{df_file}.pkl')\n")


if __name__ == "__main__":
    main()
