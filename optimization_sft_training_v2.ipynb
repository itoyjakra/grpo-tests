{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Training for Convex Optimization Exercises (v2)\n",
    "\n",
    "This notebook trains a language model on convex optimization proof problems using Supervised Fine-Tuning (SFT).\n",
    "\n",
    "**Dataset**: Boyd & Vandenberghe's \"Convex Optimization\" exercises (`exercises.jsonl`)\n",
    "\n",
    "**Key Changes in v2:**\n",
    "- ‚úÖ Train/Test Split: 10 smallest examples for testing, rest for training\n",
    "- ‚úÖ Base vs SFT Comparison: Compare base model and fine-tuned model side-by-side\n",
    "\n",
    "**Approach**: \n",
    "- Train directly on optimization exercises (proof-based problems)\n",
    "- Use reasoning tags: `<start_working_out>...<end_working_out><SOLUTION>...</SOLUTION>`\n",
    "- Multiple epochs for small dataset\n",
    "- Based on Unsloth's Qwen GRPO notebook structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"unsloth/Qwen2.5-1.5B\"\n",
    "BASE_MODEL = \"unsloth/Qwen3-4B-Base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/grpo-tests/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 12-02 23:32:50 [vllm_utils.py:700] Unsloth: Patching vLLM v1 graph capture\n",
      "==((====))==  Unsloth 2025.11.3: Fast Qwen3 patching. Transformers: 4.57.1. vLLM: 0.11.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 8. Max memory: 39.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen3-4B-Base with actual GPU utilization = 88.97%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.49 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 28.19 GB. Also swap space = 6 GB.\n",
      "Unsloth: FAILED getting compilation_config with error = cannot import name 'CompilationLevel' from 'vllm.config' (/home/ec2-user/grpo-tests/.venv/lib/python3.12/site-packages/vllm/config/__init__.py)\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-02 23:32:53 [utils.py:253] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8896558598712334, 'max_num_batched_tokens': 2048, 'max_num_seqs': 320, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': None, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'local_cache_dir': None}, 'model': 'unsloth/Qwen3-4B-Base'}\n",
      "INFO 12-02 23:32:54 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 12-02 23:32:54 [model.py:1745] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 23:32:54,842\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 23:32:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-02 23:32:55 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='unsloth/Qwen3-4B-Base', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-4B-Base, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "INFO 12-02 23:32:55 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.31.23.179:36869 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-02 23:32:55 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 12-02 23:32:56 [topk_topp_sampler.py:36] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 12-02 23:32:56 [gpu_model_runner.py:3259] Starting to load model unsloth/Qwen3-4B-Base...\n",
      "INFO 12-02 23:32:56 [cuda.py:377] Using AttentionBackendEnum.FLASHINFER backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.23s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 23:32:59 [default_loader.py:314] Loading weights took 2.13 seconds\n",
      "INFO 12-02 23:32:59 [punica_selector.py:20] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 23:32:59 [gpu_model_runner.py:3338] Model loading took 7.6334 GiB memory and 2.899625 seconds\n",
      "INFO 12-02 23:33:14 [backends.py:631] Using cache directory: /home/ec2-user/.cache/vllm/torch_compile_cache/f88e8d602b/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-02 23:33:14 [backends.py:647] Dynamo bytecode transform time: 14.25 s\n",
      "INFO 12-02 23:33:19 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.975 s\n",
      "INFO 12-02 23:33:23 [monitor.py:34] torch.compile takes 18.22 s in total\n",
      "INFO 12-02 23:33:24 [gpu_worker.py:359] Available KV cache memory: 26.83 GiB\n",
      "INFO 12-02 23:33:24 [kv_cache_utils.py:1229] GPU KV cache size: 195,360 tokens\n",
      "INFO 12-02 23:33:24 [kv_cache_utils.py:1234] Maximum concurrency for 2,048 tokens per request: 95.39x\n",
      "INFO 12-02 23:33:25 [kernel_warmup.py:65] Warming up FlashInfer attention.\n",
      "INFO 12-02 23:33:25 [vllm_utils.py:705] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-02 23:33:25 [utils.py:250] Using default LoRA kernel configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:09<00:00, 10.51it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:07<00:00, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 23:33:42 [gpu_model_runner.py:4244] Graph capturing finished in 17 secs, took 1.62 GiB\n",
      "INFO 12-02 23:33:42 [vllm_utils.py:712] Unsloth: Patched vLLM v1 graph capture finished in 17 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 23:33:43 [core.py:250] init engine (profile, create kv cache, warmup model) took 43.30 seconds\n",
      "INFO 12-02 23:33:44 [llm.py:352] Supported tasks: ('generate',)\n",
      "Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'k_norm', 'norm1', 'layer_norm1', 'post_feedforward_layernorm', 'ffn_norm', 'post_layernorm', 'pre_feedforward_layernorm', 'layer_norm2', 'input_layernorm', 'norm', 'norm2', 'attention_norm', 'q_norm']\n",
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'k_norm', 'norm1', 'layer_norm1', 'post_feedforward_layernorm', 'ffn_norm', 'cross_attn_input_layernorm', 'post_layernorm', 'pre_feedforward_layernorm', 'layer_norm2', 'input_layernorm', 'norm', 'cross_attn_post_attention_layernorm', 'norm2', 'attention_norm', 'q_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "üî• CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Can increase for longer proofs\n",
    "lora_rank = 32  # Larger rank = smarter, but slower\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Chat Template with Reasoning Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given an optimization problem.\n",
      "Think about the problem and provide your working out (proof steps).\n",
      "Place it between <start_working_out> and <end_working_out>.\n",
      "Then, provide your solution between <SOLUTION></SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given an optimization problem.\n",
    "Think about the problem and provide your working out (proof steps).\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chat template configured\n"
     ]
    }
   ],
   "source": [
    "# Create chat template\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with our specific template\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\", f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "print(\"‚úÖ Chat template configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Format Optimization Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loaded 340 optimization exercises\n",
      "\n",
      "üìã Dataset columns: ['exercise_number', 'exercise_text', 'solution_text', 'text']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exercise_number</th>\n",
       "      <th>exercise_text</th>\n",
       "      <th>solution_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Let C ‚äÜ Rn be a convex set, with x1, . . . , x...</td>\n",
       "      <td>This is readily shown by induction from the de...</td>\n",
       "      <td>2.1 Let C ‚äÜ Rn be a convex set, with x1, . . ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>Show that a set is convex if and only if its i...</td>\n",
       "      <td>We prove the Ô¨Årst part. The intersection of tw...</td>\n",
       "      <td>2.2 Show that a set is convex if and only if i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.3</td>\n",
       "      <td>Midpoint convexity. A set C is midpoint convex...</td>\n",
       "      <td>We have to show that Œ∏x + (1 ‚àí Œ∏)y ‚àà C for all...</td>\n",
       "      <td>2.3 Midpoint convexity. A set C is midpoint co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.4</td>\n",
       "      <td>Show that the convex hull of a set S is the in...</td>\n",
       "      <td>Let H be the convex hull of S and let D be the...</td>\n",
       "      <td>2.4 Show that the convex hull of a set S is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5</td>\n",
       "      <td>What is the distance between two parallel hype...</td>\n",
       "      <td>The distance between the two hyperplanes is |b...</td>\n",
       "      <td>2.5 What is the distance between two parallel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exercise_number                                      exercise_text  \\\n",
       "0             2.1  Let C ‚äÜ Rn be a convex set, with x1, . . . , x...   \n",
       "1             2.2  Show that a set is convex if and only if its i...   \n",
       "2             2.3  Midpoint convexity. A set C is midpoint convex...   \n",
       "3             2.4  Show that the convex hull of a set S is the in...   \n",
       "4             2.5  What is the distance between two parallel hype...   \n",
       "\n",
       "                                       solution_text  \\\n",
       "0  This is readily shown by induction from the de...   \n",
       "1  We prove the Ô¨Årst part. The intersection of tw...   \n",
       "2  We have to show that Œ∏x + (1 ‚àí Œ∏)y ‚àà C for all...   \n",
       "3  Let H be the convex hull of S and let D be the...   \n",
       "4  The distance between the two hyperplanes is |b...   \n",
       "\n",
       "                                                text  \n",
       "0  2.1 Let C ‚äÜ Rn be a convex set, with x1, . . ....  \n",
       "1  2.2 Show that a set is convex if and only if i...  \n",
       "2  2.3 Midpoint convexity. A set C is midpoint co...  \n",
       "3  2.4 Show that the convex hull of a set S is th...  \n",
       "4  2.5 What is the distance between two parallel ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load exercises.jsonl\n",
    "exercises = []\n",
    "with open(\"exercises.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        exercises.append(json.loads(line))\n",
    "\n",
    "dataset = pd.DataFrame(exercises)\n",
    "\n",
    "print(f\"üìö Loaded {len(dataset)} optimization exercises\")\n",
    "print(f\"\\nüìã Dataset columns: {dataset.columns.tolist()}\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Dataset with Reasoning Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset formatted with reasoning tags\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(x):\n",
    "    \"\"\"Format exercise with reasoning tags.\"\"\"\n",
    "    problem = x[\"exercise_text\"]\n",
    "    solution = x[\"solution_text\"]\n",
    "    \n",
    "    # Wrap solution with reasoning tags\n",
    "    final_prompt = \\\n",
    "        reasoning_start + solution + reasoning_end + \\\n",
    "        solution_start + \"Proven.\" + solution_end\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": problem},\n",
    "        {\"role\": \"assistant\", \"content\": final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis=1)\n",
    "print(\"‚úÖ Dataset formatted with reasoning tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Lengths and Filter by max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Length statistics:\n",
      "   Min: 108 tokens\n",
      "   Max: 2794 tokens\n",
      "   Mean: 629 tokens\n",
      "   Median: 506 tokens\n",
      "\n",
      "‚úÖ Filtered dataset:\n",
      "   Original: 340 examples\n",
      "   Kept: 336 examples\n",
      "   Removed: 4 examples (too long)\n"
     ]
    }
   ],
   "source": [
    "# Calculate token lengths\n",
    "dataset[\"N\"] = dataset[\"Messages\"].apply(\n",
    "    lambda x: len(tokenizer.apply_chat_template(x, tokenize=True))\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Length statistics:\")\n",
    "print(f\"   Min: {dataset['N'].min()} tokens\")\n",
    "print(f\"   Max: {dataset['N'].max()} tokens\")\n",
    "print(f\"   Mean: {dataset['N'].mean():.0f} tokens\")\n",
    "print(f\"   Median: {dataset['N'].median():.0f} tokens\")\n",
    "\n",
    "# Filter to examples that fit\n",
    "original_count = len(dataset)\n",
    "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Filtered dataset:\")\n",
    "print(f\"   Original: {original_count} examples\")\n",
    "print(f\"   Kept: {len(dataset)} examples\")\n",
    "print(f\"   Removed: {original_count - len(dataset)} examples (too long)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train and Test Sets\n",
    "\n",
    "**Strategy**: \n",
    "- Sort by length (shortest first)\n",
    "- Take 10 smallest examples for testing\n",
    "- Use remaining examples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Train/Test Split:\n",
      "   Test set: 10 examples (10 smallest)\n",
      "   Train set: 326 examples\n",
      "\n",
      "   Test set length range: 108-155 tokens\n",
      "   Train set length range: 155-2021 tokens\n",
      "\n",
      "üìù Test Set Examples:\n",
      "   1. [108 tokens] Find the dual cone of {Ax | x ‚™∞ 0}, where A ‚àà Rm√ón....\n",
      "   2. [113 tokens] Show that the maximum volume ellipsoid enclosed in a set is unique. Show that th...\n",
      "   3. [120 tokens] Give an example of two closed convex sets that are disjoint but cannot be strict...\n",
      "   4. [124 tokens] Show that the function f(X) = X‚àí1 is matrix convex on Sn ++....\n",
      "   5. [126 tokens] Suppose x and y are independent random vectors in Rn, with log-concave probabili...\n",
      "   6. [131 tokens] Assumptions for infeasible start Newton method. Consider the set of assumptions ...\n",
      "   7. [141 tokens] and 4.58....\n",
      "   8. [144 tokens] Functions and epigraphs. When is the epigraph of a function a halfspace? When is...\n",
      "   9. [144 tokens] Linear measurements with exponentially distributed noise. Show how to solve the ...\n",
      "   10. [155 tokens] Convex-concave game. Use the infeasible start Newton method to solve convex-conc...\n"
     ]
    }
   ],
   "source": [
    "# Sort by length (ascending)\n",
    "dataset_sorted = dataset.sort_values(by=\"N\").reset_index(drop=True)\n",
    "\n",
    "# Split: first 10 for test, rest for train\n",
    "test_dataset = dataset_sorted.iloc[:10].copy()\n",
    "train_dataset = dataset_sorted.iloc[10:].copy()\n",
    "\n",
    "print(f\"\\nüìä Train/Test Split:\")\n",
    "print(f\"   Test set: {len(test_dataset)} examples (10 smallest)\")\n",
    "print(f\"   Train set: {len(train_dataset)} examples\")\n",
    "print(f\"\\n   Test set length range: {test_dataset['N'].min()}-{test_dataset['N'].max()} tokens\")\n",
    "print(f\"   Train set length range: {train_dataset['N'].min()}-{train_dataset['N'].max()} tokens\")\n",
    "\n",
    "# Show test set examples\n",
    "print(f\"\\nüìù Test Set Examples:\")\n",
    "for i, row in test_dataset.iterrows():\n",
    "    print(f\"   {i+1}. [{row['N']} tokens] {row['exercise_text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Dataset for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training dataset prepared\n",
      "   Total examples: 326\n",
      "   Columns: ['exercise_number', 'exercise_text', 'solution_text', 'text', 'Messages', 'N']\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template to create \"text\" field for training set\n",
    "train_dataset[\"text\"] = tokenizer.apply_chat_template(\n",
    "    train_dataset[\"Messages\"].values.tolist(), \n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset_hf = Dataset.from_pandas(train_dataset)\n",
    "\n",
    "print(f\"\\n‚úÖ Training dataset prepared\")\n",
    "print(f\"   Total examples: {len(train_dataset_hf)}\")\n",
    "print(f\"   Columns: {train_dataset_hf.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Run SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training configuration:\n",
      "   Examples: 326\n",
      "   Epochs: 15\n",
      "   Batch size: 1 x 4 = 4 (effective)\n",
      "   Steps per epoch: ~81\n",
      "   Total steps: ~1215\n",
      "   Estimated time: ~10-20 minutes on modern GPU\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Calculate training steps\n",
    "num_epochs = 15\n",
    "batch_size = 1\n",
    "gradient_accumulation = 4\n",
    "steps_per_epoch = len(train_dataset_hf) // (batch_size * gradient_accumulation)\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "print(f\"üìä Training configuration:\")\n",
    "print(f\"   Examples: {len(train_dataset_hf)}\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Batch size: {batch_size} x {gradient_accumulation} = {batch_size * gradient_accumulation} (effective)\")\n",
    "print(f\"   Steps per epoch: ~{steps_per_epoch}\")\n",
    "print(f\"   Total steps: ~{total_steps}\")\n",
    "print(f\"   Estimated time: ~{total_steps * 0.5 / 60:.0f}-{total_steps * 1.0 / 60:.0f} minutes on modern GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 326/326 [00:11<00:00, 28.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset_hf,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 50,\n",
    "        num_train_epochs = 15,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/optimization_sft_v2\",\n",
    "        save_steps = 100,\n",
    "        save_total_limit = 3,\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 326 | Num Epochs = 15 | Total steps = 1,230\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 30:17, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "trainer.train()\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: unsloth/Qwen3-4B-Base_sft_model_v2/\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(f\"{BASE_MODEL}_sft_model_v2\")\n",
    "tokenizer.save_pretrained(f\"{BASE_MODEL}_sft_model_v2\")\n",
    "\n",
    "print(f\"Model saved to: {BASE_MODEL}_sft_model_v2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Base Model for Comparison\n",
    "\n",
    "We'll load the base model (without LoRA) to compare with our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 00:04:24 [vllm_utils.py:700] Unsloth: Patching vLLM v1 graph capture\n",
      "==((====))==  Unsloth 2025.11.3: Fast Qwen3 patching. Transformers: 4.57.1. vLLM: 0.11.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 8. Max memory: 39.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
      "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
      "Unsloth: vLLM loading unsloth/Qwen3-4B-Base with actual GPU utilization = 2.1%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.49 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 6 GB.\n",
      "Unsloth: FAILED getting compilation_config with error = cannot import name 'CompilationLevel' from 'vllm.config' (/home/ec2-user/grpo-tests/.venv/lib/python3.12/site-packages/vllm/config/__init__.py)\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-03 00:04:29 [utils.py:253] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.021049760301176702, 'max_num_batched_tokens': 2048, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': None, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'local_cache_dir': None}, 'model': 'unsloth/Qwen3-4B-Base'}\n",
      "INFO 12-03 00:04:29 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 12-03 00:04:29 [model.py:1745] Using max model len 256\n",
      "INFO 12-03 00:04:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-03 00:04:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='unsloth/Qwen3-4B-Base', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-4B-Base, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': None}\n",
      "Unsloth: Retrying vLLM to process 96 sequences and 2048 tokens in tandem.\n",
      "Error:\n",
      "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 22.56 MiB is free. Including non-PyTorch memory, this process has 39.46 GiB memory in use. Of the allocated memory 37.56 GiB is allocated by PyTorch, with 57.88 MiB allocated in private pools (e.g., CUDA Graphs), and 165.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "INFO 12-03 00:04:30 [utils.py:253] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.017892296256000195, 'max_num_batched_tokens': 2048, 'max_num_seqs': 96, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': None, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'local_cache_dir': None}, 'model': 'unsloth/Qwen3-4B-Base'}\n",
      "INFO 12-03 00:04:30 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 12-03 00:04:30 [model.py:1745] Using max model len 256\n",
      "INFO 12-03 00:04:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-03 00:04:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='unsloth/Qwen3-4B-Base', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-4B-Base, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 192, 'local_cache_dir': None}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 22.56 MiB is free. Including non-PyTorch memory, this process has 39.46 GiB memory in use. Of the allocated memory 37.56 GiB is allocated by PyTorch, with 57.88 MiB allocated in private pools (e.g., CUDA Graphs), and 165.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/unsloth_zoo/vllm_utils.py:1927\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args)\u001b[39m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:343\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:174\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:108\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: StatLoggerManager | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:95\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInprocClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:264\u001b[39m, in \u001b[36mInprocClient.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py:102\u001b[39m, in \u001b[36mEngineCore.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, executor_fail_callback)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Setup Model.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_fail_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py:101\u001b[39m, in \u001b[36mExecutor.__init__\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.observability_config = vllm_config.observability_config\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mself\u001b[39m.is_sleeping = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py:48\u001b[39m, in \u001b[36mUniProcExecutor._init_executor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.driver_worker.init_device()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py:273\u001b[39m, in \u001b[36mWorker.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._maybe_get_memory_pool_context(tag=\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43meep_scale_up\u001b[49m\u001b[43m=\u001b[49m\u001b[43meep_scale_up\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py:3276\u001b[39m, in \u001b[36mGPUModelRunner.load_model\u001b[39m\u001b[34m(self, eep_scale_up)\u001b[39m\n\u001b[32m   3275\u001b[39m model_loader = get_model_loader(\u001b[38;5;28mself\u001b[39m.load_config)\n\u001b[32m-> \u001b[39m\u001b[32m3276\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmodel_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\n\u001b[32m   3278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py:49\u001b[39m, in \u001b[36mBaseModelLoader.load_model\u001b[39m\u001b[34m(self, vllm_config, model_config)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLoading weights on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, load_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py:55\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(vllm_config, prefix, model_class, model_config)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config, check_compile=\u001b[38;5;28;01mTrue\u001b[39;00m, prefix=prefix):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m msg = (\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfor the design and update the model class accordingly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py:279\u001b[39m, in \u001b[36mQwen3ForCausalLM.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28mself\u001b[39m.quant_config = quant_config\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mQwen3Model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py:276\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m] = prefix\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28mself\u001b[39m.vllm_config = vllm_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py:253\u001b[39m, in \u001b[36mQwen3Model.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_layer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQwen3DecoderLayer\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py:276\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m] = prefix\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28mself\u001b[39m.vllm_config = vllm_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:337\u001b[39m, in \u001b[36mQwen2Model.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, decoder_layer_type)\u001b[39m\n\u001b[32m    336\u001b[39m decoder_layer_type = decoder_layer_type \u001b[38;5;129;01mor\u001b[39;00m Qwen2DecoderLayer\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_layer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.layers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.make_empty_intermediate_tensors = make_empty_intermediate_tensors_factory(\n\u001b[32m    349\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m], config.hidden_size\n\u001b[32m    350\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:651\u001b[39m, in \u001b[36mmake_layers\u001b[39m\u001b[34m(num_hidden_layers, layer_fn, prefix)\u001b[39m\n\u001b[32m    645\u001b[39m start_layer, end_layer = get_pp_indices(\n\u001b[32m    646\u001b[39m     num_hidden_layers, get_pp_group().rank_in_group, get_pp_group().world_size\n\u001b[32m    647\u001b[39m )\n\u001b[32m    648\u001b[39m modules = torch.nn.ModuleList(\n\u001b[32m    649\u001b[39m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)]\n\u001b[32m    650\u001b[39m     + [\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    652\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[32m    653\u001b[39m     ]\n\u001b[32m    654\u001b[39m     + [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)]\n\u001b[32m    655\u001b[39m )\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:339\u001b[39m, in \u001b[36mQwen2Model.__init__.<locals>.<lambda>\u001b[39m\u001b[34m(prefix)\u001b[39m\n\u001b[32m    336\u001b[39m decoder_layer_type = decoder_layer_type \u001b[38;5;129;01mor\u001b[39;00m Qwen2DecoderLayer\n\u001b[32m    337\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = make_layers(\n\u001b[32m    338\u001b[39m     config.num_hidden_layers,\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mdecoder_layer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    345\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.layers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m )\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.make_empty_intermediate_tensors = make_empty_intermediate_tensors_factory(\n\u001b[32m    349\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m], config.hidden_size\n\u001b[32m    350\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py:201\u001b[39m, in \u001b[36mQwen3DecoderLayer.__init__\u001b[39m\u001b[34m(self, config, cache_config, quant_config, prefix)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.self_attn = Qwen3Attention(\n\u001b[32m    186\u001b[39m     hidden_size=\u001b[38;5;28mself\u001b[39m.hidden_size,\n\u001b[32m    187\u001b[39m     num_heads=config.num_attention_heads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m     dual_chunk_attention_config=dual_chunk_attention_config,\n\u001b[32m    200\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp = \u001b[43mQwen3MLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.mlp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:91\u001b[39m, in \u001b[36mQwen2MLP.__init__\u001b[39m\u001b[34m(self, hidden_size, intermediate_size, hidden_act, quant_config, prefix)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.gate_up_proj = MergedColumnParallelLinear(\n\u001b[32m     85\u001b[39m     hidden_size,\n\u001b[32m     86\u001b[39m     [intermediate_size] * \u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.gate_up_proj\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28mself\u001b[39m.down_proj = \u001b[43mRowParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.down_proj\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden_act != \u001b[33m\"\u001b[39m\u001b[33msilu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:1312\u001b[39m, in \u001b[36mRowParallelLinear.__init__\u001b[39m\u001b[34m(self, input_size, output_size, bias, input_is_parallel, skip_bias_add, params_dtype, reduce_results, quant_config, prefix, return_bias, disable_tp)\u001b[39m\n\u001b[32m   1311\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1312\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_method\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_loader_v2\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_method\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWEIGHT_LOADER_V2_SUPPORTED\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_loader\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reduce_results \u001b[38;5;129;01mand\u001b[39;00m (bias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_bias_add):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:215\u001b[39m, in \u001b[36mUnquantizedLinearMethod.create_weights\u001b[39m\u001b[34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[39m\n\u001b[32m    213\u001b[39m weight_loader = extra_weight_attrs.pop(\u001b[33m\"\u001b[39m\u001b[33mweight_loader\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m weight = ModelWeightParameter(\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     data=\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    220\u001b[39m     input_dim=\u001b[32m1\u001b[39m,\n\u001b[32m    221\u001b[39m     output_dim=\u001b[32m0\u001b[39m,\n\u001b[32m    222\u001b[39m     weight_loader=weight_loader,\n\u001b[32m    223\u001b[39m )\n\u001b[32m    225\u001b[39m layer.register_parameter(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 22.56 MiB is free. Including non-PyTorch memory, this process has 39.46 GiB memory in use. Of the allocated memory 37.56 GiB is allocated by PyTorch, with 57.88 MiB allocated in private pools (e.g., CUDA Graphs), and 165.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load base model for comparison (no LoRA)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m base_model, base_tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Apply same chat template to base model\u001b[39;00m\n\u001b[32m     10\u001b[39m base_tokenizer.chat_template = chat_template\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/unsloth/models/loader.py:499\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[39m\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fast_inference:\n\u001b[32m    497\u001b[39m     fast_inference, model_name = fast_inference_setup(model_name, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    523\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/unsloth/models/qwen3.py:479\u001b[39m, in \u001b[36mFastQwen3Model.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(  \u001b[38;5;66;03m# TODO: Change after release\u001b[39;00m\n\u001b[32m    466\u001b[39m     model_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-7B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    477\u001b[39m     **kwargs,\n\u001b[32m    478\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastQwen3Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/unsloth/models/llama.py:2376\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, **kwargs)\u001b[39m\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2375\u001b[39m \u001b[38;5;66;03m# Load vLLM first\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m llm = \u001b[43mload_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mload_vllm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[38;5;66;03m# Convert to HF format\u001b[39;00m\n\u001b[32m   2379\u001b[39m _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-tests/.venv/lib/python3.12/site-packages/unsloth_zoo/vllm_utils.py:1941\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args)\u001b[39m\n\u001b[32m   1937\u001b[39m error = \u001b[38;5;28mstr\u001b[39m(error)\n\u001b[32m   1938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trials >= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m unsloth_vllm_standby:\n\u001b[32m   1939\u001b[39m     \u001b[38;5;66;03m# Sleep mode uses CuMemAllocator which can't run multiple instances in single process.\u001b[39;00m\n\u001b[32m   1940\u001b[39m     \u001b[38;5;66;03m# We can't do retry because vLLM will fail to load with said error.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1941\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n\u001b[32m   1943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgpu_memory_utilization\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error:\n\u001b[32m   1944\u001b[39m     approx_max_num_seqs = \u001b[38;5;28mint\u001b[39m(approx_max_num_seqs * \u001b[32m0.75\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 22.56 MiB is free. Including non-PyTorch memory, this process has 39.46 GiB memory in use. Of the allocated memory 37.56 GiB is allocated by PyTorch, with 57.88 MiB allocated in private pools (e.g., CUDA Graphs), and 165.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load base model for comparison (no LoRA)\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = True,\n",
    ")\n",
    "\n",
    "# Apply same chat template to base model\n",
    "base_tokenizer.chat_template = chat_template\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "\n",
    "print(\"‚úÖ Base model loaded for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enable Inference Mode for Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode for fine-tuned model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"‚úÖ Fine-tuned model ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Base vs SFT Comparison Function\n",
    "\n",
    "This function will:\n",
    "1. Take a test problem\n",
    "2. Generate response from base model\n",
    "3. Generate response from SFT model\n",
    "4. Display both side-by-side for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_base_vs_sft(problem: str, max_tokens: int = 1024, show_problem: bool = True):\n",
    "    \"\"\"\n",
    "    Compare base model and SFT model responses on a given problem.\n",
    "    \n",
    "    Args:\n",
    "        problem: The optimization problem text\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        show_problem: Whether to display the problem statement\n",
    "    \"\"\"\n",
    "    # Prepare messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": problem}\n",
    "    ]\n",
    "    \n",
    "    # Format prompt\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    # Show problem if requested\n",
    "    if show_problem:\n",
    "        print(\"=\"*80)\n",
    "        print(\"üìù PROBLEM\")\n",
    "        print(\"=\"*80)\n",
    "        print(problem)\n",
    "        print()\n",
    "    \n",
    "    # Generate with BASE model\n",
    "    print(\"=\"*80)\n",
    "    print(\"üîµ BASE MODEL (No Fine-tuning)\")\n",
    "    print(\"=\"*80)\n",
    "    base_inputs = base_tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        **base_inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    base_response = base_tokenizer.decode(\n",
    "        base_outputs[0][base_inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(base_response)\n",
    "    \n",
    "    # Check format for base model\n",
    "    base_has_reasoning_end = reasoning_end in base_response\n",
    "    base_has_solution_start = solution_start in base_response\n",
    "    base_has_solution_end = solution_end in base_response\n",
    "    base_format_ok = all([base_has_reasoning_end, base_has_solution_start, base_has_solution_end])\n",
    "    \n",
    "    print(f\"\\nüìä Format Check: {'‚úÖ PASS' if base_format_ok else '‚ùå FAIL'}\")\n",
    "    print(f\"   {reasoning_end}: {base_has_reasoning_end}\")\n",
    "    print(f\"   {solution_start}: {base_has_solution_start}\")\n",
    "    print(f\"   {solution_end}: {base_has_solution_end}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Generate with SFT model\n",
    "    print(\"=\"*80)\n",
    "    print(\"üü¢ SFT MODEL (Fine-tuned)\")\n",
    "    print(\"=\"*80)\n",
    "    sft_inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    sft_outputs = model.generate(\n",
    "        **sft_inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    sft_response = tokenizer.decode(\n",
    "        sft_outputs[0][sft_inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(sft_response)\n",
    "    \n",
    "    # Check format for SFT model\n",
    "    sft_has_reasoning_end = reasoning_end in sft_response\n",
    "    sft_has_solution_start = solution_start in sft_response\n",
    "    sft_has_solution_end = solution_end in sft_response\n",
    "    sft_format_ok = all([sft_has_reasoning_end, sft_has_solution_start, sft_has_solution_end])\n",
    "    \n",
    "    print(f\"\\nüìä Format Check: {'‚úÖ PASS' if sft_format_ok else '‚ùå FAIL'}\")\n",
    "    print(f\"   {reasoning_end}: {sft_has_reasoning_end}\")\n",
    "    print(f\"   {solution_start}: {sft_has_solution_start}\")\n",
    "    print(f\"   {solution_end}: {sft_has_solution_end}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìà SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Base Model Format: {'‚úÖ CORRECT' if base_format_ok else '‚ùå INCORRECT'}\")\n",
    "    print(f\"SFT Model Format:  {'‚úÖ CORRECT' if sft_format_ok else '‚ùå INCORRECT'}\")\n",
    "    \n",
    "    if sft_format_ok and not base_format_ok:\n",
    "        print(\"\\nüéâ SFT model successfully learned the format!\")\n",
    "    elif base_format_ok and sft_format_ok:\n",
    "        print(\"\\n‚úÖ Both models follow the format (compare proof quality manually)\")\n",
    "    elif not base_format_ok and not sft_format_ok:\n",
    "        print(\"\\n‚ö†Ô∏è Neither model follows the format correctly\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"‚úÖ Comparison function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test on All Test Set Examples\n",
    "\n",
    "Let's compare base vs SFT on all 10 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first example from test set\n",
    "test_problem = test_dataset.iloc[0]['exercise_text']\n",
    "print(f\"\\nüß™ Testing on: {test_dataset.iloc[0]['exercise_number']}\\n\")\n",
    "compare_base_vs_sft(test_problem, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on second example from test set\n",
    "test_problem = test_dataset.iloc[1]['exercise_text']\n",
    "print(f\"\\nüß™ Testing on: {test_dataset.iloc[1]['exercise_number']}\\n\")\n",
    "compare_base_vs_sft(test_problem, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on third example from test set\n",
    "test_problem = test_dataset.iloc[2]['exercise_text']\n",
    "print(f\"\\nüß™ Testing on: {test_dataset.iloc[2]['exercise_number']}\\n\")\n",
    "compare_base_vs_sft(test_problem, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Testing Function\n",
    "\n",
    "Use this to test any specific problem from the test set by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_by_index(idx: int, max_tokens: int = 1024):\n",
    "    \"\"\"\n",
    "    Test a specific example from the test set by index (0-9).\n",
    "    \n",
    "    Args:\n",
    "        idx: Index in test set (0-9)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \"\"\"\n",
    "    if idx < 0 or idx >= len(test_dataset):\n",
    "        print(f\"‚ùå Invalid index. Must be between 0 and {len(test_dataset)-1}\")\n",
    "        return\n",
    "    \n",
    "    problem = test_dataset.iloc[idx]['exercise_text']\n",
    "    exercise_num = test_dataset.iloc[idx]['exercise_number']\n",
    "    \n",
    "    print(f\"\\nüß™ Testing on Test Example #{idx+1}: Exercise {exercise_num}\")\n",
    "    print(f\"   Length: {test_dataset.iloc[idx]['N']} tokens\\n\")\n",
    "    \n",
    "    compare_base_vs_sft(problem, max_tokens=max_tokens)\n",
    "\n",
    "print(\"‚úÖ Interactive testing function ready\")\n",
    "print(\"\\nUsage: test_by_index(0)  # Test first example\")\n",
    "print(\"       test_by_index(5)  # Test sixth example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - test any specific index\n",
    "# Uncomment and run to test:\n",
    "# test_by_index(0)  # Test first example\n",
    "# test_by_index(5)  # Test sixth example\n",
    "# test_by_index(9)  # Test last example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Show All Test Set Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã All Test Set Problems:\\n\")\n",
    "print(\"=\"*80)\n",
    "for i, row in test_dataset.iterrows():\n",
    "    idx = test_dataset.index.get_loc(i)\n",
    "    print(f\"\\n[{idx}] Exercise {row['exercise_number']} ({row['N']} tokens)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(row['exercise_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### What We Accomplished\n",
    "- ‚úÖ Split dataset: 10 smallest for testing, rest for training\n",
    "- ‚úÖ Trained SFT model on ~326 examples\n",
    "- ‚úÖ Loaded base model for comparison\n",
    "- ‚úÖ Created comparison function to test base vs SFT\n",
    "- ‚úÖ Tested on multiple examples\n",
    "\n",
    "### Key Functions\n",
    "- `compare_base_vs_sft(problem)`: Compare base and SFT models on any problem\n",
    "- `test_by_index(idx)`: Test specific example from test set (0-9)\n",
    "\n",
    "### Model Locations\n",
    "- **SFT model**: `optimization_sft_model_v2/`\n",
    "- **Checkpoints**: `outputs/optimization_sft_v2/checkpoint-*/`\n",
    "\n",
    "### Expected Results\n",
    "- **Base model**: May not follow format, generates generic text\n",
    "- **SFT model**: Should consistently follow the reasoning tag format and produce structured proofs\n",
    "\n",
    "### Next Steps\n",
    "1. Test on all 10 test examples using `test_by_index()`\n",
    "2. Manually evaluate proof quality and correctness\n",
    "3. If quality needs improvement:\n",
    "   - Train for more epochs\n",
    "   - Try larger model (Qwen2.5-3B or 7B)\n",
    "   - Increase max_seq_length for longer proofs\n",
    "4. Consider GRPO training for further optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
